{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f1411e4-8889-450f-b79a-7472fae7d240",
   "metadata": {},
   "source": [
    "# Boosting Assignment-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6a7afd-32e1-4bb5-a366-9a3085c7e640",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10d2395-740e-46a0-9e8a-3c564262c155",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a method in machine learning where multiple decision trees are combined sequentially to make accurate predictions for continuous numerical values. Each new tree corrects the errors of the previous ones, resulting in a strong predictive model. It's powerful for handling complex relationships in data but requires careful parameter tuning and can be computationally expensive.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77495ab-3007-4b1c-8b3a-640a0ac19073",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9340818-8920-4d16-b29f-199e8ae454d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleGradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Start with the average of y as the initial prediction\n",
    "        initial_prediction = np.mean(y)\n",
    "        prediction = np.full_like(y, initial_prediction)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate residuals\n",
    "            residuals = y - prediction\n",
    "            \n",
    "            # Fit a simple tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=1)\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # Update predictions using the new tree\n",
    "            update = self.learning_rate * tree.predict(X)\n",
    "            prediction += update\n",
    "            \n",
    "            # Add the tree to the ensemble\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions by summing predictions from all trees and adding initial prediction\n",
    "        predictions = np.array([np.mean(y)] * len(X))\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b06ddd29-fabb-4633-aed8-e77df6839ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ebab6f1-e248-4bd2-a10d-e9899c42f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4166062-190c-4af3-b2f9-bce528410d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_regressor = SimpleGradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "027ce3b0-fee1-4379-af15-46679298f5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.460663409036437\n",
      "R-squared: 0.9982351041404651\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c020346-b785-4690-abe1-fa963ccae353",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa0f83f-93ec-4cf1-9be1-171722dafe9f",
   "metadata": {},
   "source": [
    "\n",
    "A weak learner in Gradient Boosting is a simple model, like a shallow decision tree, that performs slightly better than random guessing. It's used sequentially in the boosting process to correct errors made by previous models, ultimately leading to a strong predictive model when combined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90642aef-c70e-451c-9f49-e2ba860bc648",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af7dbf-bb96-42f0-8e43-117622b7d8ac",
   "metadata": {},
   "source": [
    " Gradient Boosting sequentially corrects errors made by previous models by training new models to predict the residuals. By focusing on the mistakes of the ensemble, it gradually improves predictive accuracy, combining simple models to create a strong predictor.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac932009-65b2-44f0-ace6-098660dd76d2",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748c3109-2eff-4ad3-ad19-5a29fea03447",
   "metadata": {},
   "source": [
    "Gradient Boosting builds an ensemble by sequentially training weak learners, typically decision trees, to correct errors made by the current ensemble. Each weak learner focuses on the mistakes of the previous ones, gradually improving overall predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d0b2c3-54fa-4b29-885f-e89a69d24ffa",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6549325-45ce-4307-b681-0886a32e2922",
   "metadata": {},
   "source": [
    " constructing the mathematical intuition behind Gradient Boosting involves defining a loss function, initializing predictions, computing residuals, training weak learners to predict residuals, updating predictions, and repeating iteratively. Regularization techniques can be applied for improved performance. The final prediction is a combination of all weak learners' predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c900aa-bb3e-4f2c-aee5-30e04ffe2927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
